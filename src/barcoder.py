from reader import read_file
import os, csv, sys, yaml, time, functools, builtins
from binarization import check_resilience
from flow import check_flow
from intensity_distribution_comparison import check_coarse
import numpy as np
from pathlib import Path
import matplotlib.pyplot as plt
import matplotlib.colors as mcolors
from matplotlib import gridspec
from writer import write_file, gen_combined_barcode

class MyException(Exception):
    pass

def check_channel_dim(image):
    min_intensity = np.min(image)
    mean_intensity = np.mean(image)
    return 2 * np.exp(-1) * mean_intensity <= min_intensity

def execute_htp(filepath, config_data, fail_file_loc, count, total):
    reader_data = config_data['reader']
    save_intermediates = config_data['writer']['return_intermediates']
    accept_dim_channel, accept_dim_im, channel_select, coarsening, flow, resilience, return_graphs, verbose = reader_data.values()
    r_data = config_data['resilience_parameters']
    f_data = config_data['flow_parameters']
    c_data = config_data['coarse_parameters']
    
    print = functools.partial(builtins.print, flush=True)
    vprint = print if verbose else lambda *a, **k: None

    def check(file_path, channel, resilience, flow, coarse, resilience_data, flow_data, coarse_data, fail_file_loc):
        flag = None
        figure_dir_name = remove_extension(filepath) + ' BARCODE Output'
        fig_channel_dir_name = os.path.join(figure_dir_name, 'Channel ' + str(channel))
        if not os.path.exists(figure_dir_name):
            os.makedirs(figure_dir_name)
        if not os.path.exists(fig_channel_dir_name):
            os.makedirs(fig_channel_dir_name)
        
        if resilience == True:
            r_offset = resilience_data['r_offset']
            f_step = resilience_data['frame_step']
            f_start, f_stop = resilience_data['evaluation_settings'].values()
            try:
                rfig, binarization_outputs = check_resilience(file, fig_channel_dir_name, channel, r_offset, f_step, f_start, f_stop, return_graphs, save_intermediates, verbose)
            except Exception as e:
                with open(fail_file_loc, "a", encoding="utf-8") as log_file:
                    log_file.write(f"File: {file_path}, Module: Binarization, Exception: {str(e)}\n")
                rfig = None
                binarization_outputs = [None] * 7
        else:
            rfig = None
            binarization_outputs = [None] * 7
        if flow == True:
            downsample, frame_step, frame_interval, nm_pix_ratio, win_size = flow_data.values()
            try:
                flow_outputs = check_flow(file, fig_channel_dir_name, channel, int(frame_step), downsample, frame_interval, nm_pix_ratio, return_graphs, save_intermediates, verbose, int(win_size))
            except Exception as e:
                with open(fail_file_loc, "a", encoding="utf-8") as log_file:
                    log_file.write(f"File: {file_path}, Module: Optical Flow, Exception: {str(e)}\n")
                flow_outputs = [None] * 4
        else:
            flow_outputs = [None] * 4
        if coarse == True:
            fframe, lframe = coarse_data['evaluation_settings'].values()
            percent_frames = coarse_data['mean_mode_frames_percent']
            try:
                cfig, id_outputs, flag = check_coarse(file, fig_channel_dir_name, channel, fframe, lframe, percent_frames, save_intermediates, verbose)
            except Exception as e:
                with open(fail_file_loc, "a", encoding="utf-8") as log_file:
                    log_file.write(f"File: {file_path}, Module: Intensity Distribution, Exception: {str(e)}\n")
                cfig = None
                id_outputs = [None] * 6
                flag = None
        else:
            cfig = None
            id_outputs = [None] * 6
            flag = None

        figpath = os.path.join(fig_channel_dir_name, 'Summary Graphs.png')
        if return_graphs == True:
            fig = plt.figure(figsize = (10, 5))
            gs = gridspec.GridSpec(1,2)

            if rfig != None:
                ax1 = rfig.axes[0]
                ax1.remove()
                ax1.figure = fig
                fig.add_axes(ax1)
                ax1.set_position([1.5/10, 1/10, 4/5, 4/5])

            if cfig != None:               
                ax3 = cfig.axes[0]
                ax3.remove()
                ax3.figure = fig
                fig.add_axes(ax3)
                ax3.set_position([11.5/10, 1/10, 4/5, 4/5])

            plt.savefig(figpath)
            plt.close(fig)
        plt.close('all')

        result = [channel] + [flag] + binarization_outputs + id_outputs + flow_outputs        
        vprint('Channel Screening Completed')
            
        return result
    
    file = read_file(filepath, accept_dim_im)


    if (isinstance(file, np.ndarray) == False):
        raise TypeError("File was not of the correct filetype")
    
    vprint(f"File {count} of {total}")
    channels = min(file.shape)
    
    rfc = []
    if channel_select == 'All':
        vprint('Total Channels:', channels)
        for channel in range(channels):
            vprint('Channel:', channel)
            if check_channel_dim(file[:,:,:,channel]) and not accept_dim_channel:
                vprint('Channel too dim, not enough signal, skipping...')
                continue
            elif check_channel_dim(file[:,:,:,channel]) and accept_dim_channel:
                vprint('Warning: channel is dim. Accuracy of screening may be limited by this.')
                results = check(filepath, channel, resilience, flow, coarsening, r_data, f_data, c_data, fail_file_loc)
                results[1] = results[1] + 1
                
            else:
                results = check(filepath, channel, resilience, flow, coarsening, r_data, f_data, c_data, fail_file_loc)
            rfc.append(results)
    
    else:
        while channel_select < 0:
            channel_select = channels + channel_select # -1 will correspond to last channel, etc
        if channel_select >= channels:
            channel_select = channels - 1 # Sets channel to maximum channel if channel selected is out of range
        vprint('Channel: ', channel_select)
        if check_channel_dim(file[:,:,:,channel_select]):
            vprint('Warning: channel is dim. Accuracy of screening may be limited by this.')
            results = check(filepath, channel_select, resilience, flow, coarsening, r_data, f_data, c_data, fail_file_loc)
            results[1] = results[1] + 1 # Indicate dim channel flag present
        else:
            results = check(filepath, channel_select, resilience, flow, coarsening, r_data, f_data, c_data, fail_file_loc)
        rfc.append(results)

    return rfc, count + 1

def remove_extension(filepath):
    if filepath.endswith('.tif'):
        return filepath.removesuffix('.tif')
    if filepath.endswith('.nd2'):
        return filepath.removesuffix('.nd2')

def process_directory(root_dir, config_data):
    verbose = config_data['reader']['verbose']
    writer_data = config_data['writer']
    normalize_data, _, stitch_barcode = writer_data.values()
    print = functools.partial(builtins.print, flush=True)
    vprint = print if verbose else lambda *a, **k: None
    
    if os.path.isfile(root_dir):
        all_data = []
        file_path = root_dir
        filename = os.path.basename(file_path)
        dir_name = os.path.dirname(file_path)
        rfc_data = None
        ff_loc = os.path.join(dir_name, remove_extension(filename) + "_failed_files.txt")
        open(ff_loc, 'w').close()
        time_filepath = os.path.join(dir_name, filename + 'time.txt')
        time_file = open(time_filepath, "w", encoding="utf-8")
        time_file.write(file_path + "\n")
        start_time = time.time()
        try:
            rfc_data = execute_htp(file_path, config_data, ff_loc)
        except Exception as e:
            with open(ff_loc, "a", encoding="utf-8") as log_file:
                log_file.write(f"File: {file_path}, Exception: {str(e)}\n")
        if rfc_data == None:
            raise TypeError("Please input valid file type ('.nd2', '.tiff', '.tif')")
        all_data.append([filename])
        all_data.extend(rfc_data)
        all_data.append([])
        filename = remove_extension(filename) + '_'
        end_time = time.time()
        elapsed_time = end_time - start_time
        vprint('Time Elapsed:', elapsed_time)
        time_file.write('Time Elapsed: ' + str(elapsed_time) + "\n")
        output_filepath = os.path.join(dir_name, filename + ' summary.csv')
        write_file(output_filepath, all_data)
        
        if stitch_barcode:
            output_figpath = os.path.join(dir_name, filename + ' summary barcode')
            gen_combined_barcode(np.array(rfc_data), output_figpath, normalize_data)

        settings_loc = os.path.join(dir_name, filename + " settings.yaml")
        with open(settings_loc, 'w+', encoding="utf-8") as ff:
            yaml.dump(config_data, ff)

        time_file.close()
        if os.stat(ff_loc).st_size == 0:
            os.remove(ff_loc)
    else: 
        all_data = []
        all_rfc_data = []
        time_filepath = os.path.join(root_dir, os.path.basename(root_dir) + ' time.txt')
        time_file = open(time_filepath, "w", encoding="utf-8")
        time_file.write(root_dir + "\n")
        
        start_folder_time = time.time()
        ff_loc = os.path.join(root_dir, "failed_files.txt")
        open(ff_loc, 'w').close()

        file_count = sum([len([file for file in files if (file.endswith(".tif") or file.endswith(".nd2"))]) for _, _, files in os.walk(root_dir)])
        file_itr = 1
        
        for dirpath, dirnames, filenames in os.walk(root_dir):
            dirnames[:] = [d for d in dirnames]
    
            for filename in filenames:
                if filename.startswith('._'):
                    continue

                file_path = os.path.join(dirpath, filename)
                start_time = time.time()
                try:
                    rfc_data, file_itr = execute_htp(file_path, config_data, ff_loc, file_itr, file_count)
                except TypeError:
                    continue
                except Exception as e:
                    with open(ff_loc, "a", encoding="utf-8") as log_file:
                        log_file.write(f"File: {file_path}, Exception: {str(e)}\n")
                    continue
                if rfc_data == None:
                    continue
                all_data.append([file_path])
                all_data.extend(rfc_data)
                all_data.append([])
                for result in rfc_data:
                    all_rfc_data.append(np.array(result))

                end_time = time.time()
                elapsed_time = end_time - start_time
                vprint('Time Elapsed:', elapsed_time)
                time_file.write(file_path + "\n")
                time_file.write('Time Elapsed: ' + str(elapsed_time) + "\n")
        
        output_filepath = os.path.join(root_dir, os.path.basename(root_dir) + " Summary.csv")
        try:
            write_file(output_filepath, all_data)
        except:
            counter = 1
            output_filepath = os.path.join(root_dir, os.path.basename(root_dir) + f" Summary ({counter}).csv")
            while os.path.exists(output_filepath):
                counter += 1
            write_file(output_filepath, all_data)
        
        if stitch_barcode:
            try:
                output_figpath = os.path.join(root_dir, os.path.basename(root_dir) + '_Summary Barcode')
                gen_combined_barcode(np.array(all_rfc_data), output_figpath, normalize_data)
            except Exception as e:
                with open(ff_loc, "a", encoding="utf-8") as log_file:
                    log_file.write(f"Unable to generate barcode, Exception: {str(e)}\n")

        end_folder_time = time.time()
        elapsed_folder_time = end_folder_time - start_folder_time
        vprint('Time Elapsed to Process Folder:', elapsed_folder_time)
        time_file.write('Time Elapsed to Process Folder: ' + str(elapsed_folder_time) + "\n")
        
        time_file.close()
        if os.stat(ff_loc).st_size == 0:
            os.remove(ff_loc)
        settings_loc = os.path.join(root_dir, os.path.basename(root_dir) + " Settings.yaml")
        with open(settings_loc, 'w+') as ff:
            yaml.dump(config_data, ff)